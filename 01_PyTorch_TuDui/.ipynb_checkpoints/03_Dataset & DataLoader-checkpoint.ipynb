{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9b2efa-3263-46f0-91ca-9f6f6631e945",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddca77-64bc-4bfc-a26a-6e6870621538",
   "metadata": {},
   "source": [
    "## 1. 概念和用途\n",
    "用于处理数据样本的代码可能会变得混乱且难以维护;理想情况下，我们需要我们的数据集代码与我们的模型训练代码分离，以获得更好的可读性和模块化。\n",
    "\n",
    "PyTorch 提供了两个数据抽象类：`torch.utils.data.DataLoader` 和 `torch.utils.data.Dataset` ，允许使用预加载的数据集或者自己的数据。`Dataset` 用于存储样本及其相应的标签，而 `DataLoader` 则在 `Dataset` 外部封装了一个可迭代器，以方便访问样本。 [Dataset & DataLoader 官方解释](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8aaa9-61bb-4762-bfaa-09dc03f9ebd1",
   "metadata": {},
   "source": [
    "官方解释可能比较抽象，可以结合 Python 的字典来理解。在索引字典中的元素时，其实是调用了 `__getitem__` 方法，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de4057d-14d6-45a9-8340-10844878e0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siyu\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "dic = {\n",
    "    \"name\": \"Siyu\",\n",
    "    \"age\": \"23\"\n",
    "}\n",
    "print(dic[\"name\"])\n",
    "print(dic.__getitem__(\"age\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d680114e-07cb-47fe-a985-e02f598e2789",
   "metadata": {},
   "source": [
    "那么在存储数据集时，可以采用类似字典的存储方式，然而神经网络中的数据集，在索引往往需要返回两个值：数据（data）和对应的标签（label/target），所以需要重写 `__getitem__` 方法，`Dataset` 抽象类就进行了重写。\n",
    "\n",
    "所以 `Dataset` 就是专用于实例化存放数据集对象的抽象类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0d849-a473-4061-bc5f-9468c4770fd2",
   "metadata": {},
   "source": [
    "在训练时，只有 `Dataset` 数据集并不足够，因为需要进行循环，`Dataset` 并未实现迭代器，所以就需要使用 `DataLoader` 帮助数据集能够迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e308c6c-3b72-40ac-8f9d-f7180ec51e50",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "PyTorch 为我们提供了一些已经封装好的数据集，不需要我们自己定义。CIFAR10 是 CV 训练中经常使用到的一个数据集，在 PyTorch 中 CIFAR10 是一个写好的 `Dataset`，我们使用时只需以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37592070-c3ee-4a20-bdaf-98d55a91e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchvision.datasets.CIFAR10(\"./data/\", transform=torchvision.transforms.ToTensor(), train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7881501-267f-4e20-84cb-17670c43c917",
   "metadata": {},
   "source": [
    "`datasets.CIFAR10` 就是一个 `Dataset` 子类，`data` 是这个类的一个实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2bb49-934e-40b0-aa1b-272c411aebb1",
   "metadata": {},
   "source": [
    "如果要使用自己的数据集，那么就可以自己定义数据集类。所谓数据集，其实就是一个负责处理索引(index)到样本(sample)映射的一个类(class)。\n",
    "\n",
    "`torch.utils.data.Dataset` 是一个表示数据集的抽象类。任何自定义的数据集都需要继承这个类并覆写相关方法。\n",
    "\n",
    "Pytorch提供两种数据集： Map式数据集 Iterable式数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae149cf",
   "metadata": {},
   "source": [
    "### Map Dataset\n",
    "一个Map式的数据集必须要重写 `getitem(self, index)` , `len(self)` 两个内建方法，用来表示从索引到样本的映射（Map）.\n",
    "\n",
    "这样一个数据集 dataset，举个例子，当使用 `dataset[idx]` 命令时，可以在你的硬盘中读取你的数据集中第 idx 张图片以及其标签（如果有的话）; `len(dataset)` 则会返回这个数据集的容量。\n",
    "\n",
    "自定义类大致是这样的：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1035f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):#需要继承data.Dataset\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file path or list of file names.\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        #这里需要注意的是，第一步：read one data，是一个data\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6b9c2",
   "metadata": {},
   "source": [
    "Tudui 教学中的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f13d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, img_dir: str, label: str):\n",
    "        self.__img_dir = img_dir\n",
    "        self.img_name_list = os.listdir(img_dir)\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_name_list[index]\n",
    "        img_path = os.path.join(self.__img_dir, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        label = self.label\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ec6a4",
   "metadata": {},
   "source": [
    "### Iterable Dataset\n",
    "暂时用不到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486eb18",
   "metadata": {},
   "source": [
    "### 常见的 Transforms\n",
    "torchvision.transforms : 常用的图像预处理方法，提高泛化能力，功能包括：，数据中心化，数据标准化，缩放，裁剪，旋转，翻转，填充，噪声添加，灰度变换，线性变换，仿射变换，亮度、饱和度及对比度变换。\n",
    "采用transforms.Compose()，将一系列的transforms有序组合，实现时按照这些方法依次对图像操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # 缩放\n",
    "    transforms.RandomCrop(32, padding=4),  # 随机裁剪\n",
    "    transforms.ToTensor(),  # 图片转张量，同时归一化0-255 ---》 0-1\n",
    "    transforms.Normalize(norm_mean, norm_std),  # 标准化均值为0标准差为1\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb929b2",
   "metadata": {},
   "source": [
    "官方文档：[Transforming and augmenting images](https://pytorch.org/vision/stable/transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10233f1",
   "metadata": {},
   "source": [
    "## 3. Dataloader\n",
    "> Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset. --PyTorch Documents\n",
    "\n",
    "一般来说PyTorch中深度学习训练的流程是这样的： 1. 创建Dateset 2. Dataset传递给DataLoader 3. DataLoader迭代产生训练数据提供给模型。\n",
    "\n",
    "对应的一般都会有这三部分代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011dbc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Dateset(可以自定义)\n",
    "    dataset = face_dataset # Dataset部分自定义过的face_dataset\n",
    "# Dataset传递给DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=64,shuffle=False,num_workers=8)\n",
    "# DataLoader迭代产生训练数据提供给模型\n",
    "    for i in range(epoch):\n",
    "        for index,(img,label) in dataloader:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec639ab9",
   "metadata": {},
   "source": [
    "到这里应该就 PyTorch 的数据集和数据传递机制应该就比较清晰明了了。Dataset 负责建立索引到样本的映射，DataLoader 负责以特定的方式从数据集中迭代的产生一个个 batch 的样本集合。在循环过程中实际上是 dataloader 按照其参数规定的策略调用了其 dataset 的 getitem 方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b5e1d",
   "metadata": {},
   "source": [
    "### 参数\n",
    "先看一下实例化一个DataLoader所需的参数，我们只关注几个重点即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb03d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b691cd",
   "metadata": {},
   "source": [
    "参数介绍： \n",
    "- dataset (Dataset) – 定义好的Map式或者Iterable式数据集。 \n",
    "- batch_size (python:int, optional) – 一个batch含有多少样本 (default: 1)。 \n",
    "- shuffle (bool, optional) – 每一个epoch的batch样本是相同还是随机 (default: False)。 如果设置shuffle=True，那么在下一次epoch时，会将数据打乱顺序，然后再进行下一次读取，从而两次epoch读到的数据顺序是不同的；如果设置shuffle=False，那么在下一次数据读取时，不会打乱数据的顺序，从而两次读取到的数据顺序是相同的。\n",
    "- sampler (Sampler, optional) – 决定数据集中采样的方法. 如果有，则shuffle参数必须为False。 \n",
    "- batch_sampler (Sampler, optional) – 和 sampler 类似，但是一次返回的是一个batch内所有样本的index。和 batch_size, shuffle, sampler, and drop_last 三个参数互斥。 \n",
    "- num_workers (python:int, optional) – 多少个子程序同时工作来获取数据，多线程。 (default: 0) \n",
    "- collate_fn (callable, optional) – 合并样本列表以形成小批量。 在dataloader构建的时侯，collate_fn一般是不用特殊指明的，因为默认的方法会将数据组织成我们想要的方式。但是如果想定制**batch的输出形式**的话，这个参数就非常重要了。比如在3D目标检测中，我们的batch形式希望是pointcloud_batch，gt_boxes_batch这种形式的时侯。\n",
    "- pin_memory (bool, optional) – 如果为True，数据加载器在返回前将张量复制到CUDA固定内存中。 \n",
    "- drop_last (bool, optional) – 如果数据集大小不能被batch_size整除，设置为True可删除最后一个不完整的批处理。如果设为False并且数据集的大小不能被batch_size整除，则最后一个batch将更小。(default: False) \n",
    "- timeout (numeric, optional) – 如果是正数，表明等待从worker进程中收集一个batch等待的时间，若超出设定的时间还没有收集到，那就不收集这个内容了。这个numeric应总是大于等于0。 (default: 0) \n",
    "- worker_init_fn (callable, optional*) – 每个worker初始化函数 (default: None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch221",
   "language": "python",
   "name": "pytorch221"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
